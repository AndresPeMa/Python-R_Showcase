{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa72a95",
   "metadata": {},
   "source": [
    "# Dynamic Programming with Markov Matrices\n",
    "## Andres Perez (Worked with group)\n",
    "(10/5) 5 extra points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7a83b",
   "metadata": {},
   "source": [
    "A company is considering whether to make an ad buy.  The buy costs \\\\$1 million.  \n",
    "\n",
    "In a good year, the company has sales of \\\\$4 million; in a normal year, its sales are \\\\$2 million; and in a bad year, its sales are only \\\\$1 million.  The company thinks (rightly) that an ad buy is akin to investing in its future, and it has an internal rate of return on other projects at gross rate $1+r$.\n",
    "\n",
    "If there is ***no ad buy***, the Markov matrix describing transitions between states is:\n",
    "\n",
    "$$M(0) = \n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.2 & 0\\\\\n",
    "0.2 & 0.8 & 0 \\\\\n",
    "0.1 & 0.4 & 0.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If there is ***an ad buy***, the Markov matrix describing transitions between states is:\n",
    "\n",
    "$$M(1) = \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5 & 0\\\\\n",
    "0.1 & 0.6 & 0.3 \\\\\n",
    "0.1 & 0.3 & 0.6\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "In these matrices, the first row represents a bad year, the second a normal year, and the third a good year.  The columns are likewise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88658e1a",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "#### What are the five elements of a dynamic program, as they pertain to this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea7618",
   "metadata": {},
   "source": [
    "1. The State Space is \n",
    "\n",
    "$$\n",
    "X = \\{\\textrm{Bad Year, Normal Year, Good Year} \\}\n",
    "$$\n",
    "\n",
    "2. Control Set \n",
    "\n",
    "$$\n",
    "U = \\{\\textrm{No Ad Buy, Ad Buy} \\}\n",
    "$$\n",
    "\n",
    "3. Reward Function \n",
    "\n",
    "$$r: X \\times U \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "|  | No Ad Buy  |  Ad Buy   |\n",
    "|--:-:----|------|------|\n",
    "|Bad Year     |1   |0     |\n",
    "|Normal Year  |2     |1     |\n",
    "|Good Year |4 |3 |\n",
    "\n",
    "4. Transition Rule\n",
    "\n",
    "$$M(\\textrm{No Ad Buy}) = \n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.2 & 0\\\\\n",
    "0.2 & 0.8 & 0 \\\\\n",
    "0.1 & 0.4 & 0.5\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$M(\\textrm{Ad Buy}) = \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5 & 0\\\\\n",
    "0.1 & 0.6 & 0.3 \\\\\n",
    "0.1 & 0.3 & 0.6\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "5. Discount Factor \n",
    "\n",
    "$$ \\beta = \\frac{1}{1+r}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3252823",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "#### Write the Bellman equation, when the company is having a normal year.\n",
    "\n",
    "V = Value\n",
    "\n",
    "$$V(\\mathrm{Normal}) = \\mathrm{max}\\{ 2 + \\beta[0.2\\mathrm{V(Bad)} + 0.8\\mathrm{V(Normal)}], 1 + \\beta[0.1\\mathrm{V(Bad)} + 0.6\\mathrm{V(Normal)} + 0.3\\mathrm{V(Good)}]\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ec6cc",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "#### Import numpy, pandas, and quantecon. Define the States and Controls as lists. Define the reward as a numpy array. Set 𝛽=0.9. Define the value function as an array of zeros, one for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bc591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quantecon as qe\n",
    "States = ['Bad', 'Normal', 'Good']\n",
    "Controls = ['No Buy', 'Buy']\n",
    "reward= np.array([[1,0],[2,1],[4,3]])\n",
    "beta = 0.9\n",
    "value = np.zeros(len(States))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c0fc9",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "#### Make a dictionary called transition that has two entries, indexed by 'No Buy' and 'Buy'. The entries are the two Markov matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a9b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "M0 = np.array([[0.8,0.2,0],[0.2,0.8,0],[0.1,0.4,0.5]])\n",
    "M1 = np.array([[0.5,0.5,0],[0.1,0.6,0.3],[0.1,0.3,0.6]])\n",
    "transition = {'No Buy': M0, 'Buy': M1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eabd82",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "#### Write a function that checks that the value function, the reward function, the transition rule, and the discount factor all make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "641a5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_parameters(States,Controls,reward,transition,value,beta):\n",
    "    n = len(States)\n",
    "    m = len(Controls)\n",
    "    if len(value) != n: print(\"The value function and the state space are not comformable.\")\n",
    "    if reward.shape != (n,m): print(\"The reward function is not well defined.\")\n",
    "    if len(transition) != m:\n",
    "        print(\"There is not the requisite number of transition matrices.\")\n",
    "    if not all(transition[control].shape == (n,n) for control in Controls):\n",
    "        print(\"At least one of the Markov transitions is one of the wrong dimension.\")\n",
    "    if not all(np.ones(n)@transition[control]@np.ones(n) == n for control in Controls):\n",
    "        print(\"Give me a break! One of the transitions matrices is not Markov\")\n",
    "    if not all((transition[control] >= 0).all() and (transition[control] <= 1).all() for control in Controls):\n",
    "        print(\"Give me a break! The elements of the transition matrices are not probabilities.\")\n",
    "    if beta < 0 or beta >= 1:\n",
    "        print(\"Something is wrong with the discount factor.\")\n",
    "    return (print(\"T have done the requisite checks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddccc8",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "#### Create a function that takes as inputs the state, the control, and the transition. It gives as an output the relevant transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fb0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state_probs(state,control, transition): \n",
    "    return(transition[control][States.index(state)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd84c52",
   "metadata": {},
   "source": [
    "#### Question 7\n",
    "#### Write a function that updates value function, using the Bellman relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb35e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_value(States,Controls,reward, transition, value, beta): \n",
    "    v_next =[]\n",
    "    for state in States: \n",
    "        bellman = []\n",
    "        for control in Controls: \n",
    "            bellman.append(reward[States.index(state),Controls.index(control)] + beta*(value@next_state_probs(state,control,transition)))\n",
    "        v_next.append(max(bellman))\n",
    "    return (v_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d761be7",
   "metadata": {},
   "source": [
    "#### Question 8\n",
    "#### Write a function that iterates on the value function to solve the dynamic program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da17a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(States,Controls,reward,transition,value,beta): \n",
    "    check = 1.0\n",
    "    epsilon = 10e-5\n",
    "    max_iter = 100\n",
    "    iter = 0 \n",
    "    while check > epsilon: \n",
    "        v1 = np.array(next_value(States,Controls,reward,transition,value,beta))\n",
    "        check = max(abs(value-v1))\n",
    "        value = v1\n",
    "        iter += 1 \n",
    "        if(iter > max_iter): \n",
    "            print('Algorithm did not converge')\n",
    "            break \n",
    "    return(value)\n",
    "    \n",
    "value = solve(States,Controls,reward,transition,value,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b02cd",
   "metadata": {},
   "source": [
    "#### Question 9\n",
    "#### Write a function that records the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876359d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(States,Controls,reward,transition,value,beta): \n",
    "    policy =[]\n",
    "    for state in States: \n",
    "        z=[]\n",
    "        for control in Controls:\n",
    "            z.append(reward[States.index(state),Controls.index(control)] + beta*(value@next_state_probs(state,control,transition)))\n",
    "        policy.append(Controls[z.index(max(z))])\n",
    "    return(policy)\n",
    "\n",
    "policy = optimal_policy(States,Controls,reward,transition,value,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e65f3f",
   "metadata": {},
   "source": [
    "#### Question 10\n",
    "#### Write a function that records the transition matrix, when the agent is following the optimal policy. Then compute its ergodic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b773030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_Optimal(States,transition,policy):\n",
    "    M = np.empty_like(list(transition.items())[0][1])\n",
    "    for state in States:\n",
    "        M[States.index(state),:] = transition[policy[States.index(state)]][States.index(state),:]\n",
    "    return(M)\n",
    "\n",
    "P = M_Optimal(States,transition,policy)\n",
    "from quantecon import MarkovChain \n",
    "mc = qe.MarkovChain(P)\n",
    "p_stationary = mc.stationary_distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da10dd1",
   "metadata": {},
   "source": [
    "#### Question 11\n",
    "#### Display the output as a Panda DataFrame, whose rows are indexed by the state.  The five columns should give: (1) reward if 'No buy'; (2) reward if 'Buy'; (3) the solved value function; (4) the optimal policy; and (5) the probabilities from the ergodic distribution of the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ba3001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Buy</th>\n",
       "      <th>Buy</th>\n",
       "      <th>Value Function</th>\n",
       "      <th>optimal Policy</th>\n",
       "      <th>Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.804697</td>\n",
       "      <td>No Buy</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>17.474440</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21.132976</td>\n",
       "      <td>No Buy</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No Buy  Buy  Value Function optimal Policy  Probabilities\n",
       "Bad          1    0       14.804697         No Buy       0.333333\n",
       "Normal       2    1       17.474440            Buy       0.416667\n",
       "Good         4    3       21.132976         No Buy       0.250000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PS5 = pd.DataFrame(reward,States,Controls)\n",
    "PS5['Value Function'] = value\n",
    "PS5['optimal Policy'] = policy\n",
    "PS5['Probabilities'] = p_stationary[0]\n",
    "PS5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639af3f9",
   "metadata": {},
   "source": [
    "#### Question 12\n",
    "#### Write a class called DP_Markov() that incorporates everything you have done here.  If you succeed at this task, I will give five points extra credit to everyone in your group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7f950fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Buy</th>\n",
       "      <th>Buy</th>\n",
       "      <th>Value Function</th>\n",
       "      <th>optimal Policy</th>\n",
       "      <th>Probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.804697</td>\n",
       "      <td>No Buy</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normal</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>17.474440</td>\n",
       "      <td>Buy</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21.132976</td>\n",
       "      <td>No Buy</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No Buy  Buy  Value Function optimal Policy  Probabilities\n",
       "Bad          1    0       14.804697         No Buy       0.333333\n",
       "Normal       2    1       17.474440            Buy       0.416667\n",
       "Good         4    3       21.132976         No Buy       0.250000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.zeros(len(States))\n",
    "\n",
    "class DP_Markov:\n",
    "    \n",
    "    def __init__(self, states, controls, reward, transition, value, beta=0.9):\n",
    "        n = len(States)\n",
    "        m = len(Controls)\n",
    "        if len(value) != n:\n",
    "            raise ValueError(\"The value function and the state space are not comformable.\")\n",
    "        if reward.shape != (n,m): \n",
    "            raise ValueError(\"The reward function is not well defined.\")\n",
    "        if len(transition) != m:\n",
    "            raise ValueError(\"There is not the requisite number of transition matrices.\")\n",
    "        if not all(transition[control].shape == (n,n) for control in Controls):\n",
    "            raise ValueError(\"At least one of the Markov transitions is one of the wrong dimension.\")\n",
    "        if not all(np.ones(n)@transition[control]@np.ones(n) == n for control in Controls):\n",
    "            raise ValueError(\"Give me a break! One of the transitions matrices is not Markov\")\n",
    "        if not all((transition[control] >= 0).all() and (transition[control] <= 1).all() for control in Controls):\n",
    "            raise ValueError(\"Give me a break! The elements of the transition matrices are not probabilities.\")\n",
    "        if beta < 0 or beta >= 1:\n",
    "            raise ValueError(\"Something is wrong with the discount factor.\")\n",
    "        self.states = states\n",
    "        self.controls = controls\n",
    "        self.reward = reward\n",
    "        self.transition = transition\n",
    "        self.value = value\n",
    "        self.beta = beta\n",
    "    \n",
    "    def next_state_probs(self, state, control):    \n",
    "        return(self.transition[control][self.states.index(state)])\n",
    "    \n",
    "    def next_value(self): \n",
    "        v_next =[]\n",
    "        for state in self.states: \n",
    "            bellman = []\n",
    "            for control in self.controls: \n",
    "                bellman.append(self.reward[self.states.index(state),self.controls.index(control)] + self.beta*(self.value@self.next_state_probs(state, control)))\n",
    "            v_next.append(max(bellman))\n",
    "        return (v_next)\n",
    "    \n",
    "    def solve(self): \n",
    "        check = 1.0\n",
    "        epsilon = 10e-5\n",
    "        max_iter = 100\n",
    "        iter = 0 \n",
    "        while check > epsilon: \n",
    "            v1 = np.array(self.next_value())\n",
    "            check = max(abs(self.value-v1))\n",
    "            self.value = v1\n",
    "            iter += 1 \n",
    "            if(iter > max_iter): \n",
    "                print('Algorithm did not converge')\n",
    "                break \n",
    "    \n",
    "    def optimal_policy(self): \n",
    "        policy =[]\n",
    "        for state in self.states: \n",
    "            z=[]\n",
    "            for control in self.controls:\n",
    "                z.append(self.reward[self.states.index(state),self.controls.index(control)] + self.beta*(self.value@self.next_state_probs(state, control)))\n",
    "            policy.append(Controls[z.index(max(z))])\n",
    "        return(policy)\n",
    "    \n",
    "    def M_Optimal(self):\n",
    "        M = np.empty_like(list(self.transition.items())[0][1])\n",
    "        for state in self.states:\n",
    "            M[self.states.index(state),:] = self.transition[self.optimal_policy()[self.states.index(state)]][self.states.index(state),:]\n",
    "        return(M)\n",
    "    \n",
    "    def display_optimal(self):\n",
    "        self.solve()\n",
    "        df = pd.DataFrame(self.reward,self.states,self.controls)\n",
    "        P = self.M_Optimal()\n",
    "        mc = qe.MarkovChain(P)\n",
    "        p_stationary = mc.stationary_distributions \n",
    "        df['Value Function'] = self.value\n",
    "        df['optimal Policy'] = self.optimal_policy()\n",
    "        df['Probabilities'] = p_stationary[0]\n",
    "        return(df)\n",
    "\n",
    "DP_Markov(States,Controls,reward, transition, value, beta).display_optimal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
